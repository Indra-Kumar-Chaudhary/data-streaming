Connecting CSV files into kafka 

1. Kafka Connect SpoolDir connector 

The Kafka Connect SpoolDir connector supports various flatfile formats. including CSV. 
Once you've installed it in your Kafka Connect worker make sure restart the worker for it to pick up.

You can check by running. 

    $ curl -s localhost:8083/connector-plugins | jq '.[].class' | egrep 'SpoolDir' 


2. Loading data from CSV into kafka and apply a schema
    If you have a header row wich field names you can take advantages of these to define the schema at ingestion time 
    (which is a good idea). 

    Create the connector:

        curl -i -X PUT -H "Accept:application/json" \
            -H "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-00/config \
            -d '{
                "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
                "topic": "orders_spooldir_00",
                "input.path": "/data/unprocessed",
                "finished.path": "/data/processed",
                "error.path": "/data/error",
                "input.file.pattern": ".*\\.csv",
                "schema.generation.enabled":"true",
                "csv.first.row.as.header":"true"
            }'

3. Now head over to a kafka consumer and observ our data. Here I'm using kafkacat cos it's great:

    $ docker exec kafkacat \
        kafkacat -b broker:29092 -t orders_spooldir_00 \
        -C -o-l -J \
        -s key=s -s value=avro -r http://schema-registry:8081 | \
        jq '.payload'

    # view metadata using kafkacat 
        $ sudo docker exec kafkacat kafkacat -b broker:29092 -L -J|jq 
        $ sudo docker exec kafkacat kafkacat -b broker:29092 -L -J|jq '.topics[].topic'|sort

    # View the information from specific topic 

        $ sudo docker exec kafkacat kafkacat \
            kafkacat -b broker:29092 -t orders_spooldir_00 \
            -C -o-1 -J \
            -s key=s -s value=avro -r http://schema-registry:8081 | \
            jq '.payload'


        Note: C => consume 
             -0-1 => just give laste one message

    
    # view all information 
    sudo docker exec kafkacat kafkacat \
            kafkacat -b broker:29092 -t orders_spooldir_00 \
            -C -o-1 -J \
            -s key=s -s value=avro -r http://schema-registry:8081 | \
            jq '.'

    # Delete connectors 

    curl -s "http://localhost:8083/connectors" | \
        jq '.[]' | \
        peco | \
        xargs -I{connector_name} curl -s -XDELETE "http://localhost:8083/connectors/"\{connector_name\}

    # To view connectors 

    $ curl "http://localhost:8083/connectors" | jq '.[]'


# Setting the message key 

Assuming you have header row to provide field names, you can set schema.generation.key.field the name of the field(s) you'd like 
to use for the kafka message key. If you're running this after the first example above remember that the connector 
relocates your file so you need to move it back to the input.path location for it to be processed again.


Note: The connector name (here it's source source-csv-spooldir-01) is used int tracking which files have been processed and the offset within 
them, so a connector of the same name won't reprocessed a file of the same name and lower offset than already processed. If you want to force it to reprocess a file, give the connector 
a new name. 


    curl -i -X PUT -H "Accept:application/json" \
        -H "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-01/config \
        -d '{
            "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
            "topic":"orders_spooldir_01",
            "input.path":"/data/unprocessed",
            "finished.path":"/data/processed",
            "error.path":"/data/error",
            "input.file.pattern": ".*\\.csv",
            "schema.generation.enabled":"true",
            "schema.generation.key.fields":"order_id",
            "csv.first.row.as.header":"true"

        }'


# Single Message Transformation by creating new connector 

    curl -i -X PUT -H "Accept:application/json" \
        -H "Content-Type:application/json" http://localhost:8083/connectors/source-cvs-spooldir-02/config \
        -d '{
            "connector.class":"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
            "topic":"orders_spooldir_02",
            "input.path":"/data/unprocessed",
            "finished.path":"/data/processed",
            "error.path":"/data/error",
            "input.file.pattern":".*\\.csv",
            "schema.generation.enabled":"true",
            "schema.generation.key.fields":"order_id",
            "csv.first.row.as.header":"true",
            "transforms":"catTypes",
            "transforms.catTypes.type":"org.apache.kafka.connect.transforms.Cast$Value",
            "transforms.catTypes.spec":"order_id:int32,customer_id:int32,order_total_usd:float32"
        }'

     # View Schema in schema registry 
     If you go and look at the schema thatâ€™s been created and stored in the Schema Registry you can see the field data types have been set as specified:

        
        curl --silent --location --request GET 'http://localhost:8081/subjects/orders_spooldir_02-value/versions/latest' |jq '.schema|fromjson'


# Just gimme the plain text! 

    All of the schemas seems like a bunch of fuss really, doesn't it? well not really, But if you absolutely must have CSV 
    in your kafka topic then here's how. Note that we're using different connector class and we're using 
    org.apache.kafka.connect.storage.StringConverter to write the values. 
    If you want to learn more about serializers and converters. 


    curl -i -X PUT -H "Accept:application/json" \
        -H  "Content-Type:application/json" http://localhost:8083/connectors/source-csv-spooldir-03/config \
        -d '{
            "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector",
            "value.converter":"org.apache.kafka.connect.storage.StringConverter",
            "topic": "orders_spooldir_03",
            "input.path": "/data/unprocessed",
            "finished.path": "/data/processed",
            "error.path": "/data/error",
            "input.file.pattern": ".*\\.csv"
            }'


# View message from kafka topic 
    1. To view last message. 

        sudo docker exec kafkacat kafkacat -b broker:29092 -t orders_spooldir_03 -C -o-1 
    2. view as json 
        sudo docker exec kafkacat kafkacat -b broker:29092 -t orders_spooldir_03 -C -o-1 -J
    3. View json j q 

        sudo docker exec kafkacat kafkacat -b broker:29092 -t orders_spooldir_03 -C -o-1 -j | jq '.'
    
    4. Beginning from the topic 

        sudo docker exec kafkacat kafkacat -b broker:29092 -t orders_spooldir_03 -C -o beginning -J | jq '.'

    5. Just show values 
            sudo docker exec kafkacat kafkacat -b broker:29092 -t orders_spooldir_03 -C -o beginning




## Push data from kafka topics to datase 

       curl -X PUT http://localhost:8083/connectors/sink-postgres-orders-00/config \
        -H "Content-Type: application/json" \
        -d '{
            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
            "connection.url": "jdbc:postgresql://192.168.1.9:5432/customers",
            "connection.user": "postgres-user",
            "connection.password": "postgres-pw",
            "tasks.max": "1",
            "topics": "orders_spooldir_02",
            "auto.create": "true",
            "auto.evolve":"true",
            "pk.mode":"record_value",
            "pk.fields":"order_id",
            "insert.mode": "upsert",
            "table.name.format":"orders"
        }'


CSV with Schema Example: 
    This example reads CSV files and writes them to Kafka. Ti parses them using the schema specified in 
    key.schema and value.schemas 

    1. Create a data directory and generate test data 

        curl "https://api.mockaroo.com/api/58605010?count=1000&key=25fd9c80" > "data/csv-spooldir-source.csv"

    2. Create a spooldir.properties file with the following contents 
        
        name=CsvSchemaSpoolDir
        tasks.max=1
        connector.class=com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector
        input.path=/path/to/data
        input.file.pattern=csv-spooldir-source.csv
        error.path=/path/to/error
        finished.path=/path/to/finished
        halt.on.error=false
        topic=spooldir-testing-topic
        csv.first.row.as.header=true
        key.schema={\n  \"name\" : \"com.example.users.UserKey\",\n  \"type\" : \"STRUCT\",\n  \"isOptional\" : false,\n  \"fieldSchemas\" : {\n    \"id\" : {\n      \"type\" : \"INT64\",\n      \"isOptional\" : false\n    }\n  }\n}
        value.schema={\n  \"name\" : \"com.example.users.User\",\n  \"type\" : \"STRUCT\",\n  \"isOptional\" : false,\n  \"fieldSchemas\" : {\n    \"id\" : {\n      \"type\" : \"INT64\",\n      \"isOptional\" : false\n    },\n    \"first_name\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"last_name\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"email\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"gender\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"ip_address\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"last_login\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"account_balance\" : {\n      \"name\" : \"org.apache.kafka.connect.data.Decimal\",\n      \"type\" : \"BYTES\",\n      \"version\" : 1,\n      \"parameters\" : {\n        \"scale\" : \"2\"\n      },\n      \"isOptional\" : true\n    },\n    \"country\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"favorite_color\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    }\n  }\n}


    3. Curl script to create connector 

    curl -i -X PUT -H "Accept:application/json" \
	-H "Content-Type:application/json" http://localhost:8083/connectors/source-csvSchemaSpoolDir-01/config \
	-d '{
	    "name":"CsvSchemaSpoolDir",
        "tasks.max":"1",
        "connector.class":"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "input.path":"/data/unprocessed",
        "input.file.pattern":"csv-spooldir-source.csv",
        "error.path":"/path/to/error",
        "finished.path":"/data/finished",
        "halt.on.error":"false",
        "topic":"spooldir-testing-topic",
        "csv.first.row.as.header":"true",
        "key.schema:"{\n  \"name\" : \"com.example.users.UserKey\",\n  \"type\" : \"STRUCT\",\n  \"isOptional\" : false,\n  \"fieldSchemas\" : {\n    \"id\" : {\n      \"type\" : \"INT64\",\n      \"isOptional\" : false\n    }\n  }\n}",
        "value.schema:"{\n  \"name\" : \"com.example.users.User\",\n  \"type\" : \"STRUCT\",\n  \"isOptional\" : false,\n  \"fieldSchemas\" : {\n    \"id\" : {\n      \"type\" : \"INT64\",\n      \"isOptional\" : false\n    },\n    \"first_name\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"last_name\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"email\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"gender\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"ip_address\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"last_login\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"account_balance\" : {\n      \"name\" : \"org.apache.kafka.connect.data.Decimal\",\n      \"type\" : \"BYTES\",\n      \"version\" : 1,\n      \"parameters\" : {\n        \"scale\" : \"2\"\n      },\n      \"isOptional\" : true\n    },\n    \"country\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    },\n    \"favorite_color\" : {\n      \"type\" : \"STRING\",\n      \"isOptional\" : true\n    }\n  }\n}"
	}'


    curl -i -X PUT -H "Accept:application/json" \
	-H "Content-Type:application/json" http://localhost:8083/connectors/source-csvSchemaSpoolDir-01/config \
	-d '{
        "tasks.max":"1",
        "connector.class":"com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
        "input.path":"/data/unprocessed",
        "input.file.pattern":"csv-spooldir-source.csv",
        "error.path":"/data/error",
        "finished.path":"/data/finished",
        "halt.on.error":"false",
        "topic":"spooldir-testing-topic",
        "csv.first.row.as.header":"true",
    }'